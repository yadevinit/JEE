{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ---\n# PREPARE DATA:\npromptToContinue <- function(msg){\n  msg.suffix <- \": To stop, enter no: \"\n  toContinue <- tolower(readline(paste(msg, msg.suffix)))\n  stopifnot(toContinue != \"no\")\n  # else quietly continue\n  return()\n}\ngetRowRightTot <- function(k, qaTable){\n  qaTable.k <- qaTable[k,]\n  right.k <- qaTable.k$correct # init\n  tot.k <- qaTable.k$wrong + qaTable.k$unattempted + qaTable.k$correct # init\n  if(!(is.na(qaTable.k$partCorrect1m) || is.na(qaTable.k$partCorrect2m) || is.na(qaTable.k$partCorrect3m))){\n    # has partCorrect* data. beware: is.na() doesn't consider vectors etc\n    # stopifnot(qaTable.k$markScheme == \"-201234\")\n    # \"-103\" etc don't have partCorrect* data! and there seems to be just two questions (173, 245) with partCorrect3m!\n    # if 3-marker questions have partCorrect* data, following formula has to be amended\n    right.k <- right.k + round(1/4 * qaTable.k$partCorrect1m + 2/4 * qaTable.k$partCorrect2m\n      + 3/4 * qaTable.k$partCorrect3m, 0) # fractional-marks-weighted sum\n    tot.k <- tot.k + (qaTable.k$partCorrect1m + qaTable.k$partCorrect2m + qaTable.k$partCorrect3m) # not weighted sum\n  } # else continue\n  rightTot.k <- c(as.integer(right.k), as.integer(tot.k))\n  return(rightTot.k)\n}\ngetRightTot <- function(qaTable){\n  rightTot <- mapply(getRowRightTot, c(1:nrow(qaTable)), MoreArgs=list(qaTable))\n  return(t(rightTot))\n}\nclog <- function(x) log(x + 0.5) # from ref, in case of zeroes\ncfac <- function(x, breaks = NULL) { # from ref, which by default tries to take an educated guess how to choose the\n # breaks between the categories\n if(is.null(breaks)) breaks <- unique(quantile(x, 0:10/10))\n x <- cut(x, breaks, include.lowest = TRUE, right = FALSE)\n levels(x) <- paste(breaks[-length(breaks)], ifelse(diff(breaks) > 1,\n   c(paste(\"-\", breaks[-c(1, length(breaks))] - 1, sep = \"\"), \"+\"), \"\"),\n   sep = \"\")\n return(x)\n}\ncDecimalDigits <- 3 # by default\n\nJEEdata <- read.csv( file=\"qaJEEadvanced.csv\", stringsAsFactors=TRUE) # 684 obs. of  13 variables\nstr(JEEdata)\n\nJEEdata.use <- JEEdata[,-1] # keep original as (read-in) is, as master reference, and take a copy for use\n  # eg: within(cancer, {\n  #   grade <- factor(grade, 0:2, c(\"well differentiated\",\"moderately differentiated\",\"poorly differentiated\"),\n  #     ordered = TRUE) # for ordinal data\n  #   race <- factor(...) ...})\nms.chr <- JEEdata.use$markScheme\nms.fac <- as.factor(sub(\"^2\", \"+02\", sub(\"^3\", \"+03\", ms.chr))); summary(ms.fac)\nJEEdata.use$markScheme <- ms.fac\nJEEdata.use$year <- as.factor(JEEdata.use$year)\n  # consider 2015 missing etc. Beware: not considered as ordered, which could result in different models vs unordered\nJEEdata.use$paper <- as.factor(JEEdata$paper)\nrightTot <- getRightTot(JEEdata.use); right <- rightTot[,1]; tot <- rightTot[,2]\nrOtot <- round(right / tot, cDecimalDigits)\n# wOtot <- round(JEEdata.use$wrong / tot, cDecimalDigits)\n# uOtot <- round(JEEdata.use$unattempted / tot, cDecimalDigits)\n# cOtot <- round(JEEdata.use$correct / tot, cDecimalDigits)\n# rOtot <- round(right / tot, cDecimalDigits)\n# nonwucOtot <- 1 - (wOtot + uOtot + cOtot)\n# nonwurOtot <- 1 - (wOtot + uOtot + rOtot)\n# JEEdata.use <- cbind(JEEdata.use, right=right, tot=tot, wOtot=wOtot, uOtot=uOtot, cOtot=cOtot, rOtot=rOtot)\n# JEEdata.use <- cbind(JEEdata.use, nonwucOtot=nonwucOtot, nonwurOtot=nonwurOtot)\n# wur <- JEEdata.use$wrong + JEEdata.use$unattempted + right\n# rOwu <- round(right / with(JEEdata.use, wrong + unattempted), cDecimalDigits)\nJEEdata.use <- cbind(JEEdata.use, # wu=(wur - right), # wu was named wurNotRight\n  right=right, tot=tot,\n  wOtot=round(JEEdata.use$wrong / tot, cDecimalDigits),\n  uOtot=round(JEEdata.use$unattempted / tot, cDecimalDigits),\n  cOtot=round(JEEdata.use$correct / tot, cDecimalDigits),\n  rOtot=rOtot,\n  not.rOtot=(1-rOtot) # (density of) those who got partially correct without getting anything wrong\n)\n\nJEEdata.use <- JEEdata.use[JEEdata.use$unattempted > 0,]; nrow(JEEdata.use)\n  # JEEdata[JEEdata$unattempted==0,] shows 5+2 rows from 2017 and 2012.\n  # 2017's appear to be marked correct as bonus; 2012's appear dropped\ncolNames.outCount <- c(\"wrong\", \"unattempted\", \"correct\", \"right\", \"tot\")\ncolNames.outRatio <- c(\"wOtot\", \"uOtot\", \"cOtot\", \"rOtot\", \"not.rOtot\")\n  # was: c(\"wOwur\", \"uOwur\", \"rOwur\")\nstopifnot(! (is.null(JEEdata.use[,colNames.outCount]) | is.na(JEEdata.use[,colNames.outCount])))\nstr(JEEdata.use); head(JEEdata.use)",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "'data.frame':\t684 obs. of  13 variables:\n $ qNum         : Factor w/ 114 levels \"1\",\"10\",\"11\",..: 1 12 23 34 45 56 58 59 60 2 ...\n $ unattempted  : int  38510 42122 38242 78136 52714 17632 22879 36133 37834 29613 ...\n $ wrong        : int  51117 64996 69095 50125 29914 25057 77084 104174 110251 91112 ...\n $ correct      : int  56556 25284 30124 17372 13477 48285 55195 14851 7073 34433 ...\n $ partCorrect1m: int  8975 22756 17697 9525 27122 24786 NA NA NA NA ...\n $ partCorrect2m: int  0 0 0 0 31931 39398 NA NA NA NA ...\n $ partCorrect3m: int  0 0 0 0 0 0 NA NA NA NA ...\n $ subject      : Factor w/ 3 levels \"chemistry\",\"maths\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ paper        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ year         : int  2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 ...\n $ qaNum        : int  1 2 3 4 5 6 7 8 9 10 ...\n $ qaType       : Factor w/ 7 levels \"Compr\",\"MultCorrAns\",..: 2 2 2 2 2 2 3 3 3 3 ...\n $ markScheme   : int  -201234 -201234 -201234 -201234 -201234 -201234 3 3 3 3 ...\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   -103    -104 -201234     +02     +03 \n    288      30     126      30     210 ",
            "text/latex": "\\begin{description*}\n\\item[-103] 288\n\\item[-104] 30\n\\item[-201234] 126\n\\item[+02] 30\n\\item[+03] 210\n\\end{description*}\n",
            "text/markdown": "-103\n:   288-104\n:   30-201234\n:   126+02\n:   30+03\n:   210\n\n",
            "text/html": "<dl class=dl-horizontal>\n\t<dt>-103</dt>\n\t\t<dd>288</dd>\n\t<dt>-104</dt>\n\t\t<dd>30</dd>\n\t<dt>-201234</dt>\n\t\t<dd>126</dd>\n\t<dt>+02</dt>\n\t\t<dd>30</dd>\n\t<dt>+03</dt>\n\t\t<dd>210</dd>\n</dl>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "[1] 677",
            "text/latex": "677",
            "text/markdown": "677",
            "text/html": "677"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error: !(is.null(JEEdata.use[, colNames.outCount]) | is.na(JEEdata.use[,  .... are not all TRUE\n",
          "traceback": [
            "Error: !(is.null(JEEdata.use[, colNames.outCount]) | is.na(JEEdata.use[,  .... are not all TRUE\nTraceback:\n",
            "1. stopifnot(!(is.null(JEEdata.use[, colNames.outCount]) | is.na(JEEdata.use[, \n .     colNames.outCount])))",
            "2. stop(msg, call. = FALSE, domain = NA)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ---\n# RELATE OUT:\ncProbs <- c(0, 0.01, 0.05, 0.1, 0.25, 0.333); cProbs <- sort(c(cProbs, 0.5, 1 - cProbs))\nrequire(corrplot)\nsummary(JEEdata.use)\n# for count data exploratory stat, use clog(). picture is much clearer if the dependent variable is log-transformed (just\n# as all count regression models discussed above also use a log link by default)\nwarning(\"consider log(rOtot) and other similar responses ahead\")\nfor(i in c(\"out ratios\")){ # c(\"out counts\", \"out ratios\") or c(\"out ratios\")\n  if(i==\"out counts\"){\n    cOutNames <- colNames.outCount; cDecimalDigits <- 0\n  } else {\n    cOutNames <- colNames.outRatio; cDecimalDigits <- 3\n  }\n  print(paste(\"count, mean, sd, and percentiles for outputs:\", paste(cOutNames, collapse=\", \")))\n  for(outName in cOutNames){\n    out <- JEEdata.use[,outName]\n    out.stat <- c(length(out), round(mean(out), cDecimalDigits),\n      round(sd(out), cDecimalDigits), round(quantile(out, probs=cProbs), cDecimalDigits))\n    print(out.stat)\n    hist(out, xlab=outName)\n    promptToContinue(\"continue\")\n  }\n\n  cor.meth <- \"spearman\"\n    # Kendall's tau or Spearman's rho statistic is used to estimate a rank-based measure of association. These are more\n    # robust and have been recommended if the data do not necessarily come from a bivariate normal distribution.\n    # method=c(\"pearson\", \"spearman\", \"kendall\").\n  myCor <- cor(JEEdata.use[,cOutNames], method=cor.meth)\n    # consider right+unattempted as OUT\n  print(round(myCor, 2)) # coz range [-1,1]\n  corrplot(myCor, type=\"upper\", order=\"hclust\", # for just the upper triangular of correlation matrix\n    title=paste(\"Correlation Plot using method\", cor.meth, \"with\", paste(cOutNames, collapse=\",\")))\n    # alt: require(\"PerformanceAnalytics\"); chart.Correlation(my_data, histogram=TRUE, pch=19)\n  promptToContinue(\"continue\")\n}\noutRatio.fac <- with(JEEdata.use,\n  cbind(as.data.frame(rOtot), wOtot.fac=as.factor(cfac(wOtot)), uOtot.fac=as.factor(cfac(uOtot)),\n    cOtot.fac=as.factor(cfac(cOtot)), rOtot.fac=as.factor(cfac(rOtot)), not.rOtot.fac=as.factor(cfac(not.rOtot))))\nplot(rOtot ~ wOtot.fac + uOtot.fac + cOtot.fac + rOtot.fac + not.rOtot.fac, data=outRatio.fac)\n# outCount.fac <- with(JEEdata.use,\n#   cbind(as.data.frame(right), w.fac=as.factor(cfac(wrong)), u.fac=as.factor(cfac(unattempted)),\n#     c.fac=as.factor(cfac(correct)), r.fac=as.factor(cfac(right))))\n# plot(right ~ w.fac + u.fac + c.fac, data=outCount.fac)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ---\n# RELATE IN:\nwith(JEEdata.use, table(qaType, year)) # table(exclude=) levels to remove for all factors in ...\nwith(JEEdata.use, table(qaType, subject))\nqaType.grp <- JEEdata.use$qaType\nlevels(qaType.grp) <- c(\"notMultSingCorrDigit\", \"MultCorrAns\", \"notMultSingCorrDigit\", \"notMultSingCorrDigit\",\n  \"SingCorrAns\", \"SingDigitInteger\", \"notMultSingCorrDigit\")\n  # c(\"Compr\", \"MultCorrAns\", \"NumAns\", \"ParaSingleAns\", \"SingCorrAns\", \"SingDigitInteger\", \"TwoListMatchSingleAns\")\n  # considering year-wise samples: {\"MultCorrAns\", \"SingCorrAns\", \"SingDigitInteger\", others}\n  # qaType is really unordered. so, ordering might be unsuitable\n  # JEEdata.use$qaType <- ordered(JEEdata.use$qaType, levels=c(\"NumAns\", \"MultCorrAns\", \"SingDigitInteger\",\n  #   \"ParaSingleAns\", \"Compr\", \"SingCorrAns\", \"TwoListMatchSingleAns\"))\n  # <- relevel(ml$prog, ref = \"academic\") # choose the level of our outcome that we wish to use as our baseline\nwith(JEEdata.use, table(markScheme, year)) # -ve and +ve markScheme could be grouped, but no finer coz biased sample\nmarkScheme.grp <- JEEdata.use$markScheme\nlevels(markScheme.grp) <- c(\"-ve\", \"-ve\", \"-ve\", \"+ve\", \"+ve\") # {-103 -104 -201234} {+02 +03} and no finer\nJEEdata.use <- cbind(JEEdata.use, qaType.grp=qaType.grp, markScheme.grp=markScheme.grp)\n# qat.sub.rOwu <- round(exp(xtabs(log(rOwu) ~ qaType + subject, data=JEEdata.use) /\n#   with(JEEdata.use, table(qaType, subject))), cDecimalDigits)\n# !is.list(replications(formula, JEEdata)) # eg, replications(~ . - yield, npk).  test for balance\n# most non-experimentally-designed data is unbalanced\n# lme()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ---\n# MODEL OUT-IN:\ncInNames <- c(\"subject\", \"paper\", \"year\", \"qaType\", \"qaType.grp\", \"markScheme.grp\")\nsummary(JEEdata.use[,cInNames])\n# cInNames <- cInNames[-(2:2)]\ncOutNames <- \"right\" # \"log(rOwu)\"\nmdl.spec <- as.formula(paste(cOutNames, \"~\", paste(cInNames, collapse=\" + \"))); mdl.spec\nmdl.spec.plot <- update(mdl.spec, log(.) ~ .); mdl.spec.plot # replace LHS with log(LHS)\nplot(mdl.spec.plot, data=JEEdata.use)\n# plot(log(rOwu) ~ qaType * markScheme.grp, data=JEEdata.use)\n\n  # ref: https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf\n  require(MASS); require(lmtest); require(sandwich); require(pscl)\n  myData <- JEEdata.use; mdl.spec.bak <- mdl.spec\n  contrasts(myData$qaType) <- contr.treatment(levels(myData$qaType), base=3) # \"NumAns\" coz that goes with lowest mean\n  contrasts(myData$subject) <- contr.treatment(levels(myData$subject), base=2) # \"maths\" coz that goes with lowest mean\n  # contr.treatment(n, base=1, contrasts=TRUE, sparse=FALSE); C()\n  # ?glm says:\n  # Non-NULL weights can be used to indicate that different observations have different dispersions (with the values in\n  # weights being inversely proportional to the dispersions); or equivalently, when the elements of weights are positive\n  # integers w_i, that each response y_i is the mean of w_i unit-weight observations. For a binomial GLM prior weights are\n  # used to give the number of trials when the response is the proportion of successes: they would rarely be used for a\n  # Poisson GLM.\n  mdl.spec <- update(mdl.spec, ~ . + offset(log(tot)))\n    # append offset(log(tot)) to indicate term with a fixed coefficient of one\n\n  fit <- glm(mdl.spec, data=myData, family=poisson)\n    # prefer rate of incidence (not count)\n    # =loglm() cf Negative Binomial, possibly with Hurdle and Zero-Inflated Regression Models. package pscl?\n    # family=negative.binomial(link=\"log\")\n    # Count data often have an exposure variable, which indicates the number of times the event could have happened.\n    # This variable should be incorporated into a Poisson model with the use of the offset option.\n  coeftest(fit, vcov=sandwich); summary(fit); plot(fit)\n\n  fit <- glm(update(mdl.spec, ~ . - qaType.grp), data=myData, family=poisson)\n    # drop qaType.grp coz NAs due to related qaType.grp: [Coefficients: (3 not defined because of singularities)]\n  coeftest(fit, vcov=sandwich); summary(fit); plot(fit)\n\n  fit <- MASS::glm.nb(mdl.spec, data=myData); m1 <- fit\n  coeftest(fit, vcov=sandwich); summary(fit); plot(fit)\n    # 2018::physics 1.7x r count as per NB\n    # myData[c(44, 49, 66, ?99 if -subject?),] are plotted as outliers in\n    # Residuals vs Fitted, Q-Q, & Scale-Location plots; investigate\n    # myData[c(61, 261, 594),] are plotted as outliers in Residuals vs Leverage plot; investigate\n    # prefer rate of incidence (not count). was: offset(log(wur))\n    # ref https://mac-theobio.github.io/QMEE/Generalized_linear_models.html says of overdispersion:\n    # \"too much variance: (residual deviance)/(residual df) should be Ëœ1. (Ratio >1.2 worrisome; ratio>3,\n    # v. worrisome (check your model & data!)\"\n    # so, 715.6/661=1.08 is therefore \"not worrisome\"; Dispersion parameter 2.89\n    # ref https://mac-theobio.github.io/QMEE/Generalized_linear_models.html says of offsets:\n    # \"constant terms added to a model\n    # what if we want to model densities rather than counts?\n    # log-link (Poisson/NB) models: mu0=exp(beta0+beta1x1+...)\n    # if we know the area then we want mu=A * mu0\n    # equivalent to adding log(A) to the linear predictor (exp(log(mu0)+log(A))=mu0 * A)\n    # use ... + offset(log(A)) in R formula\"\n    # so, by including offset(log(tot)), density of 'right' (over an \"area\" of 'tot' JEE candidates) is being modeled\n\n  fit <- MASS::glm.nb(update(mdl.spec, ~ . - qaType.grp), data=myData)\n    # drop qaType.grp coz NAs due to related qaType.grp: [Coefficients: (3 not defined because of singularities)]\n  coeftest(fit, vcov=sandwich); summary(fit); plot(fit)\n\n  fit <- MASS::glm.nb(update(mdl.spec, ~ . - qaType.grp - 1), data=myData)\n    # drop qaType.grp coz NAs due to related qaType.grp: [Coefficients: (3 not defined because of singularities)]\n    # drop intercept coz we want to estimate relative impacts of variables, not necessarily estimate the response itself\n    #   (~ + 0) or (~ . - 1) can equivalently drop the intercept\n  coeftest(fit, vcov=sandwich); summary(fit); plot(fit)\n    # Null Deviance went up from 880 to 3394 after dropping intercept from model spec, while AIC stays 15434\n\n  fit <- MASS::glm.nb(update(mdl.spec, ~ . - qaType.grp - 1 - paper), data=myData); m2 <- fit\n    # drop paper too coz insignificant impact\n  coeftest(fit, vcov=sandwich); summary(fit); plot(fit)\n\n  est.betahat <- round(exp(cbind(Estimate=coef(fit), confint(fit))), cDecimalDigits); est.betahat\n  # ref: https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/\n  # We might be interested in looking at incident rate ratios rather than coefficients. So, exponentiate\n  # incident rate (IRR) for levelA is Est times the incident rate for the reference group\n  # For a constant value of other variables, for every one-point increase in chosen variable (in the following),\n  # estimated odds of modeled response are multiplied by ...  exp(betahat) iff logit link for Odds Ratio\n  # betahat <- fit$coefficients; round(exp(betahat), cDecimalDigits)\n  # The two degree-of-freedom chi-square test if < alpha=0.05 indicates that factor is a statistically significant\n  # predictor of response\n  fit <- MASS::glm.nb(update(mdl.spec, ~ . - qaType.grp - 1 - paper - subject), data=myData); m3 <- fit\n  anova(m1, m2, m3) # factor subject is stat-significant predictor of response; so, avoid dropping subject, and prefer m2.\n    # dropping paper and qaType.grp is ok coz they are not significant predictors\n  fit <- m2 # preferred model\n\n  # TBD: additionally, you can predict (and plot) for a data set to get what the model generates:\n  # lines(myYear, predict(fit, type=\"response\", list(year=myYear)))\n  fit.pred.link <- predict(fit, type=\"link\"); fit.pred.resp <- predict(fit, type=\"response\")\n  str(fit.pred.link); str(fit.pred.resp)\n\n  # fit <- glm(rOtot ~ ., weights=tot, family=binomial(link=\"logit\"))\n    # As a numerical vector with values between 0 and 1, interpreted as the proportion of successful cases (with the\n    # total number of cases given by the weights)\n  # fit <- glm(cbind(right, (tot - right)) ~ ., data=myData, weights=tot, family=binomial(link=\"logit\"))\n    # As a two-column integer matrix: the first column gives the number of successes and the second the number of failures\n    # weights when response has counts, e.g., for binomial GLM, prior weights are used to give number of trials",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# investigate outliers from diagnostic plots and theorize on extreme observations, e.g., 44, 66, etc\n# ref http://www.contrib.andrew.cmu.edu/~achoulde/94842/homework/regression_diagnostics.html.\n# [[Those \"Cook's distance\" dashed curves don't even appear on the plot.]]\n# else the points that lie close to or outside of the dashed red curves are worth investigating further.\nmyData.o1 <- myData[c(44, 49, 66),] # are plotted as outliers in Residuals vs Fitted, Q-Q, & Scale-Location plots\nmyData.o2 <- myData[c(61, 261, 594),] # are plotted as outliers in Residuals vs Leverage plot\nmyData.o1; myData.o2\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "r",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.5.3",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}